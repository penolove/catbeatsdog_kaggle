{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [sec1] Data resize by openCV and write out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "datapath=\"/home/stream/Downloads/\"\n",
    "target=\"ResizeCDD64/\"\n",
    "source=\"cat_dog_data/\"\n",
    "currentdata=\"cat_dog_testdata/\"\n",
    "pic_list=glob.glob(datapath+source+currentdata+\"*.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pic_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in xrange(len(pic_list)):\n",
    "    img=cv2.resize(cv2.imread(pic_list[k]),(64,64))\n",
    "    cv2.imwrite(datapath+target+currentdata+pic_list[k].split('/')[-1], img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# load datas to numpy type, and write to npy file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "outfile_x.npy : images that stores in ndarray with 64*64*3\n",
    "\n",
    "outfile_y.npy : labels of outfile_x.npy i.e. cat/dog using 0, 1 presents\n",
    "\n",
    "https://drive.google.com/open?id=0BxRU7n3UdSpqR2YxeDdwVW5uLVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "datapath=\"/home/stream/Downloads/\"\n",
    "currentdata=\"cat_dog_data_all/cat/\"\n",
    "pic_list_cat=glob.glob(datapath+currentdata+\"*.jpg\")\n",
    "\n",
    "currentdata=\"cat_dog_data_all/dog/\"\n",
    "pic_list_dog=glob.glob(datapath+currentdata+\"*.jpg\")\n",
    "\n",
    "datarows=len(pic_list_cat)+len(pic_list_dog)\n",
    "frames = np.empty((datarows, 64,64, 3))\n",
    "frames2 = np.empty((datarows, 64,64, 3))\n",
    "y_train=np.empty(datarows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#path print\n",
    "print pic_list_cat[0]\n",
    "print pic_list_dog[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k=0\n",
    "print cv2.cvtColor(cv2.resize(cv2.imread(pic_list_cat[k]),(64,64)), cv2.cv.CV_BGR2RGB).shape\n",
    "print cv2.cvtColor(cv2.resize(cv2.imread(pic_list_cat[k]),(64,64)), cv2.cv.CV_BGR2RGB).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using Opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count=0\n",
    "#img = cv2.cvtColor(image, cv2.cv.CV_BGR2RGB)\n",
    "\n",
    "for k in xrange(len(pic_list_cat)):\n",
    "    frames[k,:,:,:] = cv2.cvtColor(cv2.resize(cv2.imread(pic_list_cat[k]),(64,64)), cv2.cv.CV_BGR2RGB)\n",
    "y_train[count:count+len(pic_list_cat)]=0\n",
    "count=count+len(pic_list_cat)\n",
    "\n",
    "for k in xrange(len(pic_list_dog)):\n",
    "    frames[count+k,:,:,:] =  cv2.cvtColor(cv2.resize(cv2.imread(pic_list_dog[k]),(64,64)), cv2.cv.CV_BGR2RGB)\n",
    "y_train[count:count+len(pic_list_dog)]=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "(64, 64, 3)\n",
      "3\n",
      "float64\n",
      "255.0\n"
     ]
    }
   ],
   "source": [
    "print type(frames[0])\n",
    "print frames[0].shape\n",
    "print frames[0].ndim\n",
    "print frames[0].dtype\n",
    "print frames[0].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import skimage\n",
    "import skimage.io\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename=pic_list_cat[0]\n",
    "color=True\n",
    "img = resize(skimage.io.imread(filename, as_grey=not color),(64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print type(img)\n",
    "print img.shape\n",
    "print img.ndim\n",
    "print img.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count=0\n",
    "\n",
    "for k in xrange(len(pic_list_cat)):\n",
    "    frames[k,:,:,:] = skimage.img_as_float(resize(skimage.io.imread(pic_list_cat[k], as_grey=not color),(64,64))).astype(np.float32)\n",
    "y_train[count:count+len(pic_list_cat)]=0\n",
    "count=count+len(pic_list_cat)\n",
    "\n",
    "for k in xrange(len(pic_list_dog)):\n",
    "    frames[count+k,:,:,:] =  skimage.img_as_float(resize(skimage.io.imread(pic_list_dog[k], as_grey=not color),(64,64))).astype(np.float32)\n",
    "y_train[count:count+len(pic_list_dog)]=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"/home/stream/whimh2.0/Bibi/outfile_x\",frames)\n",
    "np.save(\"/home/stream/whimh2.0/Bibi/outfile_y\",y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [sec2] train a easy logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load preprocessed data\n",
    "feature_x=np.load(\"/home/stream/whimh2.0/Bibi/outfile_x.npy\")\n",
    "y_train=np.load(\"/home/stream/whimh2.0/Bibi/outfile_y.npy\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create training set and testing set\n",
    "arr = np.arange(len(feature_x))\n",
    "np.random.shuffle(arr)\n",
    "x_train=feature_x[arr]\n",
    "y_train=y_train[arr]\n",
    "data={\n",
    "  'X_train': x_train[:int(len(x_train)*0.8)],\n",
    "  'y_train': y_train[:int(len(x_train)*0.8)],\n",
    "  'X_val': x_train[int(len(x_train)*0.8):],\n",
    "  'y_val': y_train[int(len(x_train)*0.8):],\n",
    "}\n",
    "print \"there are \"+ str(data['X_train'].shape[0]) + \" images in training set\"\n",
    "print \"there are \"+ str(data['X_val'].shape[0]) + \" images in testing set\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a Ez tensorflow logistregression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the session first\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prepare placeholder for X and Y (all these operation will directly place into default graph in sess)\n",
    "x = tf.placeholder(tf.float32, [None, 64,64,3])\n",
    "y_tf = tf.placeholder(tf.int32, shape=[None,],name='truth_y')\n",
    "#for y which is 0,1 needed to be one_hot encoding\n",
    "y_tf_2=tf.one_hot(y_tf,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a fc to image and output to 2dim\n",
    "W = tf.Variable(tf.zeros([64*64*3, 2]))\n",
    "b = tf.Variable(tf.zeros([2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_flat = tf.reshape(x, [-1, 64*64*3])\n",
    "y = tf.nn.softmax(tf.matmul(x_flat, W) + b)\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_tf_2 * tf.log(y), reduction_indices=[1]))\n",
    "regularizers = (tf.nn.l2_loss(W))\n",
    "loss=cross_entropy+2e-1*regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reduce the loss by applying adam \n",
    "train_step = tf.train.AdamOptimizer(1e-6).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a mesurement for accuracy\n",
    "with tf.name_scope('measure'):\n",
    "    with tf.name_scope('predict'):\n",
    "        predctions=tf.argmax(y,1, name='predictions_')\n",
    "    with tf.name_scope('groundtruth'):\n",
    "        ground_truth=tf.argmax(y_tf_2,1)\n",
    "    correct_prediction = tf.equal(predctions, ground_truth)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#initialize all these variable\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k=0\n",
    "for i in range(10000):\n",
    "    arr=np.arange(len(data['X_train']))\n",
    "    np.random.shuffle(arr)\n",
    "    idx = arr[:50]\n",
    "    train_step.run(feed_dict={x:data['X_train'][idx], y_tf: data['y_train'][idx]})\n",
    "    if i%1000==0:\n",
    "        train_accuracy=sess.run(accuracy, feed_dict={x:data['X_train'], y_tf: data['y_train']})\n",
    "        testing_accuracy=sess.run(accuracy, feed_dict={x:data['X_val'], y_tf: data['y_val']})\n",
    "        print(\"step %d, training accuracy %g, testing acc %g\"%(i, train_accuracy,testing_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by applying logistic regression can't get a good result.. ~57%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [sec3] applying a simple CNN model\n",
    "3x3(WxH) x3(chanel) filter *32 - pooling - 3x3(WxH) x3 *64 - pooling -fc [1024] -dropout -fc [2]-softmax- cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#load preprocessed data\n",
    "feature_x=np.load(\"/home/stream/whimh2.0/Bibi/outfile_x.npy\")\n",
    "y_train=np.load(\"/home/stream/whimh2.0/Bibi/outfile_y.npy\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create training set and testing set\n",
    "arr = np.arange(len(feature_x))\n",
    "np.random.shuffle(arr)\n",
    "x_train=feature_x[arr]\n",
    "y_train=y_train[arr]\n",
    "data={\n",
    "  'X_train': x_train[:int(len(x_train)*0.8)],\n",
    "  'y_train': y_train[:int(len(x_train)*0.8)],\n",
    "  'X_val': x_train[int(len(x_train)*0.8):],\n",
    "  'y_val': y_train[int(len(x_train)*0.8):],\n",
    "}\n",
    "print \"there are \"+ str(data['X_train'].shape[0]) + \" images in training set\"\n",
    "print \"there are \"+ str(data['X_val'].shape[0]) + \" images in testing set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#used to create weight\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.0001)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.01, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#used to perform conv\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('conv'):\n",
    "    x_image1 = tf.placeholder(tf.float32, shape=[None, 64,64,3])\n",
    "    x_image=tf.image.resize_images(x_image1,32,32)\n",
    "    y_tf = tf.placeholder(tf.int32, shape=[None,],name='truth_y')\n",
    "    #for y which is 0,1 needed to be one_hot encoding\n",
    "    y_tf_2=tf.one_hot(y_tf,2)\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    W_conv1 = weight_variable([3,3, 3, 32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "    # first conv1\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "    \n",
    "    W_conv2 = weight_variable([3, 3, 32, 64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "    W_fc1 = weight_variable([8 * 8 * 64, 1024])\n",
    "    b_fc1 = bias_variable([1024])\n",
    "with tf.name_scope('Fc'):\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 8*8*64])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "    #here a dropout layer\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "    #two output\n",
    "    W_fc2 = weight_variable([1024, 2])\n",
    "    b_fc2 = bias_variable([2])\n",
    "\n",
    "    y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "    #cross_entropy\n",
    "    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_tf_2 * tf.log(y_conv), reduction_indices=[1]))\n",
    "    #regularizers\n",
    "    regularizers = (tf.nn.l2_loss(W_conv2) + tf.nn.l2_loss(W_conv1)+\n",
    "                    (tf.nn.l2_loss(W_fc1) + tf.nn.l2_loss(W_fc2)))\n",
    "    regularTerm = tf.placeholder(tf.float32)\n",
    "    learingrate = tf.placeholder(tf.float32)\n",
    "\n",
    "    #loss\n",
    "    loss=cross_entropy+regularTerm*regularizers\n",
    "    #Adam Optimizer\n",
    "    train_step = tf.train.AdamOptimizer(learingrate).minimize(loss)\n",
    "    #measurement\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_tf_2,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_pool2\n",
    "#check the size for pool to fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k=0\n",
    "for i in range(80000):\n",
    "    arr=np.arange(len(data['X_train']))\n",
    "    np.random.shuffle(arr)\n",
    "    idx = arr[:40]\n",
    "    train_step.run(feed_dict={x_image1:data['X_train'][idx], y_tf: data['y_train'][idx],keep_prob:0.3,regularTerm:1e-4,learingrate:1e-5})\n",
    "    if i%1000==0:\n",
    "        train_accuracy=sess.run(accuracy, feed_dict={x_image1:data['X_train'], y_tf: data['y_train'],keep_prob:1})\n",
    "        testing_accuracy=sess.run(accuracy, feed_dict={x_image1:data['X_val'], y_tf: data['y_val'],keep_prob:1})\n",
    "        print(\"step %d, training accuracy %g, testing acc %g\"%(i, train_accuracy,testing_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(cross_entropy, feed_dict={x_image1:data['X_val'], y_tf: data['y_val'],keep_prob:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(cross_entropy, feed_dict={x_image1:data['X_train'], y_tf: data['y_train'],keep_prob:1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "using all images get a better result => 81.5%~ learning rate 1e-5 ,regularzation 1e-4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [sec4] using Pretrained model\n",
    "https://github.com/ry/tensorflow-vgg16\n",
    "VGG_ILSVRC_16_layers tfmodel\n",
    "\n",
    "finetune.npy :pool:5 of all images\n",
    "can be found in https://drive.google.com/open?id=0BxRU7n3UdSpqR2YxeDdwVW5uLVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [sec4-1] forward to pool:5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#load the graph_def from disk\n",
    "with open(\"/home/stream/tensorflow-vgg16/vgg16-20160129.tfmodel\", mode='rb') as f:\n",
    "  fileContent = f.read()\n",
    "graph_def = tf.GraphDef()\n",
    "graph_def.ParseFromString(fileContent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# insert graph from vggface16 model\n",
    "tf.import_graph_def(graph_def,name='vggILSVRC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let's check it's form first\n",
    "train_writer = tf.train.SummaryWriter('/tmp/loser1/train',sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just a vgg16\n",
    "<img src=\"img/BibiPre1.png\",width=200,height=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# close it to clear graph\n",
    "tf.reset_default_graph()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# restart session\n",
    "sess = tf.InteractiveSession()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph loaded into default graph\n"
     ]
    }
   ],
   "source": [
    "#let froward it to pooling5 for all images\n",
    "x_tf = tf.placeholder(tf.float32, shape=[None, 64,64,3])\n",
    "#since the original image is 224x224 , we can resize it as the same or smaller(64x64)\n",
    "#just 32x32 will fails in some layers(since polling/conv).\n",
    "#x_tf_1=tf.image.resize_images(x_tf,64,64)\n",
    "x_tf_1=tf.image.resize_images(x_tf,224,224)\n",
    "\n",
    "y_tf = tf.placeholder(tf.int32, shape=[None,])\n",
    "# import the graph_def to default graph , replace images with x_tf_1/x_tf\n",
    "tf.import_graph_def(graph_def, input_map={ \"images\": x_tf_1 })\n",
    "print \"graph loaded into default graph\"\n",
    "\n",
    "graph = tf.get_default_graph()\n",
    "#[i.name for i in graph.get_operations()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#[i.name for i in graph.get_operations()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the pooling5\n",
    "feature_net=graph.get_tensor_by_name(\"import/pool5:0\")\n",
    "#get the pooling4\n",
    "#feature_net=graph.get_tensor_by_name(\"import/conv5_2/Relu:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'import/pool5:0' shape=(?, 7, 7, 512) dtype=float32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load preprocessed data\n",
    "frames=np.load(\"/home/stream/whimh2.0/Bibi/outfile_x.npy\")\n",
    "y_train=np.load(\"/home/stream/whimh2.0/Bibi/outfile_y.npy\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#it takes ~20GB ram for per 100 image forward\n",
    "feature_x = sess.run([feature_net], feed_dict={x_tf:frames[:100], y_tf: y_train[:100]})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current i: 10\n",
      "current i: 20\n",
      "current i: 30\n",
      "current i: 40\n",
      "current i: 50\n",
      "current i: 60\n",
      "current i: 70\n",
      "current i: 80\n",
      "current i: 90\n",
      "current i: 100\n",
      "current i: 110\n",
      "current i: 120\n",
      "current i: 130\n",
      "current i: 140\n",
      "current i: 150\n",
      "current i: 160\n",
      "current i: 170\n",
      "current i: 180\n",
      "current i: 190\n",
      "current i: 200\n",
      "current i: 210\n",
      "current i: 220\n",
      "current i: 230\n",
      "current i: 240\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,250):\n",
    "    if i%10==0:\n",
    "        print \"current i: \"+str(i)\n",
    "    feature_temp = sess.run([feature_net], feed_dict={x_tf:frames[i*100:(i+1)*100], y_tf: y_train[i*100:(i+1)*100]})[0]\n",
    "    feature_x=np.concatenate((feature_x, feature_temp), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#np.save(\"/home/stream/whimh2.0/Bibi/finetune_conv5_2\",feature_x)\n",
    "np.save(\"/home/stream/whimh2.0/Bibi/finetune_vgg_pool5\",feature_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 7, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "print feature_x.shape\n",
    "#clear the graph\n",
    "tf.reset_default_graph()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "del feature_x\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [sec4-2] Train the model using pool:5 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_x=np.load(\"/home/stream/whimh2.0/Bibi/finetune_vgg_pool5.npy\")\n",
    "#frames=np.load(\"/home/stream/whimh2.0/Bibi/outfile_x.npy\")\n",
    "y_train=np.load(\"/home/stream/whimh2.0/Bibi/outfile_y.npy\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 20000 images in training set\n",
      "there are 5000 images in testing set\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.preprocessing import normalize\n",
    "#norm2 = normalize(feature_x, axis=1)\n",
    "arr = np.arange(len(feature_x))\n",
    "np.random.shuffle(arr)\n",
    "np.save(\"/home/stream/whimh2.0/Bibi/20161020seed\",arr)\n",
    "x_train=feature_x[arr]\n",
    "#x_train=norm2[arr]\n",
    "y_train=y_train[arr]\n",
    "data={\n",
    "  'X_train': x_train[:int(len(x_train)*0.8)],\n",
    "  'y_train': y_train[:int(len(x_train)*0.8)],\n",
    "  'X_val': x_train[int(len(x_train)*0.8):],\n",
    "  'y_val': y_train[int(len(x_train)*0.8):],\n",
    "}\n",
    "print \"there are \"+ str(data['X_train'].shape[0]) + \" images in training set\"\n",
    "print \"there are \"+ str(data['X_val'].shape[0]) + \" images in testing set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.0001)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.01, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#x_tfed = tf.placeholder(tf.float32, shape=[None,2,2,512],name='feature_x')\n",
    "x_tfed = tf.placeholder(tf.float32, shape=[None,7,7,512],name='feature_x')\n",
    "y_tf = tf.placeholder(tf.int32, shape=[None,],name='truth_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Here contains the finetune layers\n",
    "with tf.name_scope('fintune_whimh'):\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    regularTerm = tf.placeholder(tf.float32)\n",
    "    learningRate = tf.placeholder(tf.float32)\n",
    "\n",
    "    with tf.name_scope('Fc1'):\n",
    "        #calculate_entropy\n",
    "        h_pool3_flat = tf.reshape(x_tfed, [-1, 7*7*512])\n",
    "        W_whimh_fc1 = weight_variable([7*7*512,1024])\n",
    "        b_whimh_fc1 = bias_variable([1024])\n",
    "        h_fc1=tf.nn.relu(tf.matmul(h_pool3_flat, W_whimh_fc1) + b_whimh_fc1) \n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "    with tf.name_scope('Fc2'):\n",
    "        #calculate_entropy\n",
    "        W_whimh_fc2 = weight_variable([1024, 2])\n",
    "        b_whimh_fc2 = bias_variable([2])\n",
    "        y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_whimh_fc2) + b_whimh_fc2, name='predictions_softmax')\n",
    "        #y_conv = tf.nn.dropout(h_fc2, keep_prob)\n",
    "\n",
    "#    with tf.name_scope('Fc3'):\n",
    "#        #calculate_entropy\n",
    "#        W_whimh_fc3 = weight_variable([1024, 2])\n",
    "#        b_whimh_fc3 = bias_variable([2])\n",
    "#        y_conv=tf.nn.softmax(tf.matmul(h_fc2_drop, W_whimh_fc3) + b_whimh_fc3, name='predictions_softmax') \n",
    "    y_tf_2=tf.one_hot(y_tf,2)\n",
    "    with tf.name_scope('loss_calulate'):\n",
    "        #calculate_entropy\n",
    "        cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_tf_2 * tf.log(y_conv), reduction_indices=[1]))\n",
    "    with tf.name_scope('regular'):    \n",
    "        regularizers = (tf.nn.l2_loss(W_whimh_fc1) + tf.nn.l2_loss(b_whimh_fc1) +\n",
    "                        tf.nn.l2_loss(W_whimh_fc2) + tf.nn.l2_loss(b_whimh_fc2) )\n",
    "                        #tf.nn.l2_loss(W_whimh_fc3) + tf.nn.l2_loss(b_whimh_fc3) )\n",
    "    with tf.name_scope('loss'):\n",
    "        #carefully deal overfitting\n",
    "        loss=cross_entropy+regularTerm*regularizers\n",
    "    with tf.name_scope('slover'):\n",
    "        train_step = tf.train.AdamOptimizer(learningRate).minimize(loss)\n",
    "    with tf.name_scope('measure'):\n",
    "        with tf.name_scope('predict'):\n",
    "            predctions=tf.argmax(y_conv,1, name='predictions_')\n",
    "        with tf.name_scope('groundtruth'):\n",
    "            ground_truth=tf.argmax(y_tf_2,1)\n",
    "        correct_prediction = tf.equal(predctions, ground_truth)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testing_accuracy,testing_cross_entropy = sess.run([accuracy,cross_entropy], feed_dict={   \n",
    "    x_tfed:data['X_val'], y_tf: data['y_val'],keep_prob:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mini_testing_cross_entropy=testing_cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum entropy : 0.141104\n"
     ]
    }
   ],
   "source": [
    "print(\"minimum entropy : %g\"%mini_testing_cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 100, testing acc 0.884, cross_entropy 0.482654\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 200, testing acc 0.9028, cross_entropy 0.345979\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 300, testing acc 0.9092, cross_entropy 0.277073\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 400, testing acc 0.9178, cross_entropy 0.238784\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 500, training accuracy 0.9325, testing acc 0.9208 \n",
      ",=== cross_entropy 0.215044 ===\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 600, testing acc 0.9236, cross_entropy 0.199973\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 700, testing acc 0.926, cross_entropy 0.189663\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 800, testing acc 0.9256, cross_entropy 0.183156\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 900, testing acc 0.929, cross_entropy 0.176733\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 1000, training accuracy 0.9507, testing acc 0.9316 \n",
      ",=== cross_entropy 0.172066 ===\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 1100, testing acc 0.9314, cross_entropy 0.168557\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 1200, testing acc 0.9318, cross_entropy 0.1656\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 1300, testing acc 0.9338, cross_entropy 0.162104\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 1400, testing acc 0.9346, cross_entropy 0.15912\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 1500, training accuracy 0.95775, testing acc 0.9368 \n",
      ",=== cross_entropy 0.157079 ===\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 1600, testing acc 0.9352, cross_entropy 0.156721\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 1700, testing acc 0.9368, cross_entropy 0.153512\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 1800, testing acc 0.9378, cross_entropy 0.151059\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 1900, testing acc 0.9378, cross_entropy 0.151332\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 2000, training accuracy 0.96405, testing acc 0.9378 \n",
      ",=== cross_entropy 0.150018 ===\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 2100, testing acc 0.9382, cross_entropy 0.148935\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 2200, testing acc 0.938, cross_entropy 0.150287\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 2300, testing acc 0.9378, cross_entropy 0.149183\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 2400, testing acc 0.9388, cross_entropy 0.14673\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 2500, training accuracy 0.9696, testing acc 0.9386 \n",
      ",=== cross_entropy 0.146445 ===\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 2600, testing acc 0.9396, cross_entropy 0.145971\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 2700, testing acc 0.9398, cross_entropy 0.145022\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 2800, testing acc 0.9406, cross_entropy 0.144722\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 2900, testing acc 0.9386, cross_entropy 0.145435\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 3000, training accuracy 0.975, testing acc 0.939 \n",
      ",=== cross_entropy 0.14355 ===\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 3100, testing acc 0.939, cross_entropy 0.143987\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 3200, testing acc 0.9392, cross_entropy 0.145799\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 3300, testing acc 0.9396, cross_entropy 0.146616\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 3400, testing acc 0.9398, cross_entropy 0.143991\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 3500, training accuracy 0.9782, testing acc 0.9404 \n",
      ",=== cross_entropy 0.144362 ===\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 3600, testing acc 0.9394, cross_entropy 0.148409\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 3700, testing acc 0.9406, cross_entropy 0.146103\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 3800, testing acc 0.939, cross_entropy 0.144372\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 3900, testing acc 0.9402, cross_entropy 0.14473\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 4000, training accuracy 0.98075, testing acc 0.9414 \n",
      ",=== cross_entropy 0.144429 ===\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 4100, testing acc 0.9416, cross_entropy 0.144076\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 4200, testing acc 0.941, cross_entropy 0.144059\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 4300, testing acc 0.9408, cross_entropy 0.144611\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "**************minimum entropy updated 0.142479 **************\n",
      "step 4400, testing acc 0.9414, cross_entropy 0.142479\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 4500, training accuracy 0.98445, testing acc 0.9432 \n",
      ",=== cross_entropy 0.14398 ===\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "**************minimum entropy updated 0.141215 **************\n",
      "step 4600, testing acc 0.9418, cross_entropy 0.141215\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "**************minimum entropy updated 0.141104 **************\n",
      "step 4700, testing acc 0.9426, cross_entropy 0.141104\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 4800, testing acc 0.943, cross_entropy 0.142164\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 4900, testing acc 0.939, cross_entropy 0.152699\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 5000, training accuracy 0.98695, testing acc 0.941 \n",
      ",=== cross_entropy 0.145683 ===\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 5100, testing acc 0.9414, cross_entropy 0.14401\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 5200, testing acc 0.9436, cross_entropy 0.143078\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 5300, testing acc 0.9414, cross_entropy 0.14216\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 5400, testing acc 0.9404, cross_entropy 0.151952\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 5500, training accuracy 0.9895, testing acc 0.9426 \n",
      ",=== cross_entropy 0.143431 ===\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 5600, testing acc 0.944, cross_entropy 0.14568\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 5700, testing acc 0.9426, cross_entropy 0.145949\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 5800, testing acc 0.9426, cross_entropy 0.144786\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 5900, testing acc 0.9436, cross_entropy 0.146649\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 6000, training accuracy 0.99145, testing acc 0.9444 \n",
      ",=== cross_entropy 0.144527 ===\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 6100, testing acc 0.9442, cross_entropy 0.146914\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 6200, testing acc 0.9454, cross_entropy 0.144949\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 6300, testing acc 0.9444, cross_entropy 0.148167\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 6400, testing acc 0.9438, cross_entropy 0.147974\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 6500, training accuracy 0.9932, testing acc 0.9442 \n",
      ",=== cross_entropy 0.148814 ===\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 6600, testing acc 0.9432, cross_entropy 0.151309\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 6700, testing acc 0.9428, cross_entropy 0.153289\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 6800, testing acc 0.942, cross_entropy 0.151518\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 6900, testing acc 0.9408, cross_entropy 0.153852\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 7000, training accuracy 0.9942, testing acc 0.9442 \n",
      ",=== cross_entropy 0.151819 ===\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 7100, testing acc 0.9434, cross_entropy 0.149749\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 7200, testing acc 0.9416, cross_entropy 0.154054\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 7300, testing acc 0.9426, cross_entropy 0.153637\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 7400, testing acc 0.9428, cross_entropy 0.1569\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 7500, training accuracy 0.9953, testing acc 0.9412 \n",
      ",=== cross_entropy 0.156038 ===\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 7600, testing acc 0.9416, cross_entropy 0.157122\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 7700, testing acc 0.9422, cross_entropy 0.155957\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 7800, testing acc 0.9424, cross_entropy 0.15774\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 7900, testing acc 0.9428, cross_entropy 0.15968\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 8000, training accuracy 0.99605, testing acc 0.943 \n",
      ",=== cross_entropy 0.158212 ===\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 8100, testing acc 0.943, cross_entropy 0.156707\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 8200, testing acc 0.942, cross_entropy 0.164076\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 8300, testing acc 0.9442, cross_entropy 0.160505\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 8400, testing acc 0.9432, cross_entropy 0.163311\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 8500, training accuracy 0.99675, testing acc 0.9434 \n",
      ",=== cross_entropy 0.161323 ===\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 8600, testing acc 0.9432, cross_entropy 0.162451\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 8700, testing acc 0.9428, cross_entropy 0.162541\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 8800, testing acc 0.9432, cross_entropy 0.163921\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 8900, testing acc 0.9434, cross_entropy 0.165238\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 9000, training accuracy 0.998, testing acc 0.943 \n",
      ",=== cross_entropy 0.162985 ===\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 9100, testing acc 0.9434, cross_entropy 0.167397\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 9200, testing acc 0.941, cross_entropy 0.170166\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 9300, testing acc 0.9416, cross_entropy 0.170657\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 9400, testing acc 0.9406, cross_entropy 0.168764\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 9500, training accuracy 0.99825, testing acc 0.9412 \n",
      ",=== cross_entropy 0.171205 ===\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 9600, testing acc 0.9416, cross_entropy 0.16962\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 9700, testing acc 0.942, cross_entropy 0.17057\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 9800, testing acc 0.9406, cross_entropy 0.173316\n",
      "measurement\n",
      "==========Weight have been decaied 1e-07==============\n",
      "step 9900, testing acc 0.9424, cross_entropy 0.170871\n",
      "test accuracy 0.9388\n"
     ]
    }
   ],
   "source": [
    "k=0\n",
    "WeiDecayCount=0\n",
    "gamma=0.08\n",
    "BaseLearningR=1e-7\n",
    "#maxinterval=1\n",
    "#interval=0\n",
    "\n",
    "learningR=BaseLearningR\n",
    "for i in range(1,10000):\n",
    "  arr=np.arange(len(data['X_train']))\n",
    "  np.random.shuffle(arr)\n",
    "  idx = arr[:30]\n",
    "  train_step.run(feed_dict={x_tfed:data['X_train'][idx], y_tf: data['y_train'][idx],keep_prob:0.5,learningRate:learningR,regularTerm:2e-3})\n",
    "  \n",
    "  if i%100==0:\n",
    "    print(\"measurement\")\n",
    "    #interval+=1\n",
    "    ############testing acc#########\n",
    "    testing_accuracy,testing_cross_entropy = sess.run([accuracy,cross_entropy], feed_dict={   \n",
    "        x_tfed:data['X_val'], y_tf: data['y_val'],keep_prob:1})\n",
    "    ############ Wegiht Decay #########\n",
    "    #WeiDecayCount+=1\n",
    "    learningR=BaseLearningR*np.exp(-gamma*WeiDecayCount)\n",
    "    print(\"==========Weight have been decaied %g==============\"%learningR)\n",
    "\n",
    "    if(mini_testing_cross_entropy>testing_cross_entropy):\n",
    "        #interval=0\n",
    "        mini_testing_cross_entropy=testing_cross_entropy\n",
    "        saver=tf.train.Saver(tf.all_variables())\n",
    "        saver.save(sess,\"vgg_pool5.ckpt\")\n",
    "        print(\"**************minimum entropy updated %g **************\"%mini_testing_cross_entropy)\n",
    "    ############training acc###########\n",
    "    if i%500==0:\n",
    "        train_accuracy = sess.run(accuracy, feed_dict={   \n",
    "                x_tfed:data['X_train'], y_tf: data['y_train'],keep_prob:1})\n",
    "        print(\"step %d, training accuracy %g, testing acc %g \\n,=== cross_entropy %g ===\"%(i, train_accuracy,testing_accuracy,testing_cross_entropy))\n",
    "    else:\n",
    "        print(\"step %d, testing acc %g, cross_entropy %g\"%(i,testing_accuracy,testing_cross_entropy))\n",
    "        \n",
    "#overall\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x_tfed:data['X_val'], y_tf: data['y_val'],keep_prob:1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13136297"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using [7*7*512 x 1024]-[1024 x2] two Fc can achieve ~0.945 accuracy \n",
    "#cross_entropy is about ~0.15\n",
    "sess.run(cross_entropy, feed_dict={x_tfed:data['X_val'], y_tf: data['y_val'],keep_prob:1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vggface16.ckpt'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver=tf.train.Saver(tf.all_variables())\n",
    "saver.save(sess,\"vggface16.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [sec5]Tune from resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "sess = tf.InteractiveSession()\n",
    "graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load preprocessed data\n",
    "feature_x=np.load(\"/home/stream/whimh2.0/Bibi/outfile_x.npy\")\n",
    "y_train=np.load(\"/home/stream/whimh2.0/Bibi/outfile_y.npy\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_tf = tf.placeholder(tf.float32, shape=[None, 64,64,3])\n",
    "#since the original image is 224x224 , we can resize it as the same or smaller(64x64)\n",
    "#just 32x32 will fails in some layers(since polling/conv).\n",
    "#x_tf_1=tf.image.resize_images(x_tf,64,64)\n",
    "x_tf_1=tf.image.resize_images(x_tf,224,224)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frames=sess.run(x_tf_1, feed_dict={x_tf:(feature_x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del feature_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newsaver=tf.train.import_meta_graph('/home/stream/tensorflow-resnet-pretrained-20160509/ResNet-L50.meta')\n",
    "newsaver.restore(sess,'/home/stream/tensorflow-resnet-pretrained-20160509/ResNet-L50.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save the current graph and see what happens\n",
    "train_writer = tf.train.SummaryWriter('/tmp/resnet/train',sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.get_default_graph()\n",
    "#[i.name for i in graph.get_operations()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_net=graph.get_tensor_by_name(\"scale5/block3/Relu:0\")\n",
    "#feature_net=graph.get_tensor_by_name(\"mul:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images=graph.get_tensor_by_name(\"images:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take a look at this\n",
    "print images\n",
    "print feature_net\n",
    "print frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_x=sess.run(feature_net, feed_dict={images:(frames[:1000]/255)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print feature_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time\n",
    "for i in range(1,25):\n",
    "    if i%5==0:\n",
    "        print \"current i: \"+str(i)\n",
    "    feature_temp = sess.run([feature_net], feed_dict={images:frames[i*1000:(i+1)*1000]/255})[0]\n",
    "    feature_x=np.concatenate((feature_x, feature_temp), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.save(\"/home/stream/whimh2.0/Bibi/finetune_res_50_scale5\",feature_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [sec5-1]Tune from resnet-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_x=np.load(\"/home/stream/whimh2.0/Bibi/finetune_res_50_scale5.npy\")\n",
    "y_train=np.load(\"/home/stream/whimh2.0/Bibi/outfile_y.npy\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 7, 7, 2048)\n"
     ]
    }
   ],
   "source": [
    "print feature_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 20000 images in training set\n",
      "there are 5000 images in testing set\n"
     ]
    }
   ],
   "source": [
    "#arr = np.arange(len(feature_x))\n",
    "#np.random.shuffle(arr)\n",
    "arr=np.load(\"/home/stream/whimh2.0/Bibi/20161020seed.npy\")\n",
    "\n",
    "x_train=feature_x[arr]\n",
    "y_train=y_train[arr]\n",
    "data={\n",
    "  'X_train': x_train[:int(len(x_train)*0.8)],\n",
    "  'y_train': y_train[:int(len(x_train)*0.8)],\n",
    "  'X_val': x_train[int(len(x_train)*0.8):],\n",
    "  'y_val': y_train[int(len(x_train)*0.8):],\n",
    "}\n",
    "print \"there are \"+ str(data['X_train'].shape[0]) + \" images in training set\"\n",
    "print \"there are \"+ str(data['X_val'].shape[0]) + \" images in testing set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del feature_x\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.0001)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.01, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_tfed = tf.placeholder(tf.float32, shape=[None,7,7,2048],name='feature_x')\n",
    "y_tf = tf.placeholder(tf.int32, shape=[None,],name='truth_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Here contains the finetune layers\n",
    "with tf.name_scope('fintune_whimh'):\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    regularTerm = tf.placeholder(tf.float32)\n",
    "    learningRate = tf.placeholder(tf.float32)\n",
    "\n",
    "    with tf.name_scope('Fc1'):\n",
    "        #calculate_entropy\n",
    "        h_pool3_flat = tf.reshape(x_tfed, [-1, 7*7*2048])\n",
    "        W_whimh_fc1 = weight_variable([7*7*2048,1024])\n",
    "        b_whimh_fc1 = bias_variable([1024])\n",
    "        h_fc1=tf.nn.relu(tf.matmul(h_pool3_flat, W_whimh_fc1) + b_whimh_fc1) \n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "    with tf.name_scope('Fc2'):\n",
    "        #calculate_entropy\n",
    "        W_whimh_fc2 = weight_variable([1024, 2])\n",
    "        b_whimh_fc2 = bias_variable([2])\n",
    "        y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_whimh_fc2) + b_whimh_fc2, name='predictions_softmax')\n",
    "        #y_conv = tf.nn.dropout(h_fc2, keep_prob)\n",
    "\n",
    "#    with tf.name_scope('Fc3'):\n",
    "#        #calculate_entropy\n",
    "#        W_whimh_fc3 = weight_variable([1024, 2])\n",
    "#        b_whimh_fc3 = bias_variable([2])\n",
    "#        y_conv=tf.nn.softmax(tf.matmul(h_fc2_drop, W_whimh_fc3) + b_whimh_fc3, name='predictions_softmax') \n",
    "    y_tf_2=tf.one_hot(y_tf,2)\n",
    "    with tf.name_scope('loss_calulate'):\n",
    "        #calculate_entropy\n",
    "        cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_tf_2 * tf.log(y_conv), reduction_indices=[1]))\n",
    "    with tf.name_scope('regular'):    \n",
    "        regularizers = (tf.nn.l2_loss(W_whimh_fc1) + tf.nn.l2_loss(b_whimh_fc1) +\n",
    "                        tf.nn.l2_loss(W_whimh_fc2) + tf.nn.l2_loss(b_whimh_fc2) )\n",
    "                        #tf.nn.l2_loss(W_whimh_fc3) + tf.nn.l2_loss(b_whimh_fc3) )\n",
    "    with tf.name_scope('loss'):\n",
    "        #carefully deal overfitting\n",
    "        loss=cross_entropy+regularTerm*regularizers\n",
    "    with tf.name_scope('slover'):\n",
    "        train_step = tf.train.AdamOptimizer(learningRate).minimize(loss)\n",
    "    with tf.name_scope('measure'):\n",
    "        with tf.name_scope('predict'):\n",
    "            predctions=tf.argmax(y_conv,1, name='predictions_')\n",
    "        with tf.name_scope('groundtruth'):\n",
    "            ground_truth=tf.argmax(y_tf_2,1)\n",
    "        correct_prediction = tf.equal(predctions, ground_truth)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testing_accuracy,testing_cross_entropy = sess.run([accuracy,cross_entropy], feed_dict={   \n",
    "    x_tfed:data['X_val'], y_tf: data['y_val'],keep_prob:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mini_testing_cross_entropy=testing_cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum entropy : 0.693145\n"
     ]
    }
   ],
   "source": [
    "print(\"minimum entropy : %g\"%mini_testing_cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "measurement\n",
      "==========Weight have been decaied 1.66634e-07==============\n",
      "step 100, testing acc 0.9656, cross_entropy 0.0804957\n",
      "measurement\n",
      "==========Weight have been decaied 1.63334e-07==============\n",
      "step 200, testing acc 0.9664, cross_entropy 0.0804013\n",
      "measurement\n",
      "==========Weight have been decaied 1.601e-07==============\n",
      "step 300, testing acc 0.967, cross_entropy 0.080311\n",
      "measurement\n",
      "==========Weight have been decaied 1.5693e-07==============\n",
      "step 400, testing acc 0.9668, cross_entropy 0.0803166\n",
      "measurement\n",
      "==========Weight have been decaied 1.53822e-07==============\n",
      "step 500, testing acc 0.9668, cross_entropy 0.080424\n",
      "measurement\n",
      "==========Weight have been decaied 1.50776e-07==============\n",
      "step 600, testing acc 0.9666, cross_entropy 0.0804444\n",
      "measurement\n",
      "==========Weight have been decaied 1.47791e-07==============\n",
      "step 700, testing acc 0.9658, cross_entropy 0.0805089\n",
      "measurement\n",
      "==========Weight have been decaied 1.44864e-07==============\n",
      "step 800, testing acc 0.9666, cross_entropy 0.0803288\n",
      "measurement\n",
      "==========Weight have been decaied 1.41996e-07==============\n",
      "step 900, testing acc 0.9666, cross_entropy 0.0803515\n",
      "measurement\n",
      "==========Weight have been decaied 1.39184e-07==============\n",
      "step 1000, training accuracy 0.9885, testing acc 0.9666 \n",
      ",=== cross_entropy 0.0803089 ===\n",
      "measurement\n",
      "==========Weight have been decaied 1.36428e-07==============\n",
      "**************minimum entropy updated 0.0801874 **************\n",
      "step 1100, testing acc 0.9666, cross_entropy 0.0801874\n",
      "measurement\n",
      "==========Weight have been decaied 1.33727e-07==============\n",
      "**************minimum entropy updated 0.0801679 **************\n",
      "step 1200, testing acc 0.9666, cross_entropy 0.0801679\n",
      "measurement\n",
      "==========Weight have been decaied 1.31079e-07==============\n",
      "step 1300, testing acc 0.9662, cross_entropy 0.0802371\n",
      "measurement\n",
      "==========Weight have been decaied 1.28483e-07==============\n",
      "step 1400, testing acc 0.9666, cross_entropy 0.0803104\n",
      "measurement\n",
      "==========Weight have been decaied 1.25939e-07==============\n",
      "step 1500, testing acc 0.9662, cross_entropy 0.0802627\n",
      "measurement\n",
      "==========Weight have been decaied 1.23445e-07==============\n",
      "step 1600, testing acc 0.9666, cross_entropy 0.080291\n",
      "measurement\n",
      "==========Weight have been decaied 1.21001e-07==============\n",
      "step 1700, testing acc 0.9664, cross_entropy 0.0802184\n",
      "measurement\n",
      "==========Weight have been decaied 1.18605e-07==============\n",
      "**************minimum entropy updated 0.0800301 **************\n",
      "step 1800, testing acc 0.9668, cross_entropy 0.0800301\n",
      "measurement\n",
      "==========Weight have been decaied 1.16256e-07==============\n",
      "step 1900, testing acc 0.9662, cross_entropy 0.0801118\n",
      "measurement\n",
      "==========Weight have been decaied 1.13954e-07==============\n",
      "step 2000, training accuracy 0.9892, testing acc 0.9662 \n",
      ",=== cross_entropy 0.0800885 ===\n",
      "measurement\n",
      "==========Weight have been decaied 1.11698e-07==============\n",
      "step 2100, testing acc 0.9664, cross_entropy 0.0801942\n",
      "measurement\n",
      "==========Weight have been decaied 1.09486e-07==============\n",
      "step 2200, testing acc 0.9666, cross_entropy 0.080317\n",
      "measurement\n",
      "==========Weight have been decaied 1.07318e-07==============\n",
      "step 2300, testing acc 0.9666, cross_entropy 0.0803286\n",
      "measurement\n",
      "==========Weight have been decaied 1.05193e-07==============\n",
      "step 2400, testing acc 0.9666, cross_entropy 0.0803376\n",
      "measurement\n",
      "==========Weight have been decaied 1.0311e-07==============\n",
      "step 2500, testing acc 0.966, cross_entropy 0.0803944\n",
      "measurement\n",
      "==========Weight have been decaied 1.01068e-07==============\n",
      "step 2600, testing acc 0.9658, cross_entropy 0.0804259\n",
      "measurement\n",
      "==========Weight have been decaied 9.90672e-08==============\n",
      "step 2700, testing acc 0.966, cross_entropy 0.0801949\n",
      "measurement\n",
      "==========Weight have been decaied 9.71055e-08==============\n",
      "step 2800, testing acc 0.966, cross_entropy 0.0802398\n",
      "measurement\n",
      "==========Weight have been decaied 9.51827e-08==============\n",
      "step 2900, testing acc 0.9658, cross_entropy 0.080206\n",
      "measurement\n",
      "==========Weight have been decaied 9.3298e-08==============\n",
      "step 3000, training accuracy 0.98945, testing acc 0.9662 \n",
      ",=== cross_entropy 0.0801432 ===\n",
      "measurement\n",
      "==========Weight have been decaied 9.14506e-08==============\n",
      "step 3100, testing acc 0.9662, cross_entropy 0.0801254\n",
      "measurement\n",
      "==========Weight have been decaied 8.96397e-08==============\n",
      "step 3200, testing acc 0.9662, cross_entropy 0.0800359\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-4c3f92ec0754>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m   \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m   \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m   \u001b[0mtrain_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx_tfed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'X_train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_tf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y_train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearningRate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlearningR\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mregularTerm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1e-3\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/stream/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m   1551\u001b[0m         \u001b[0mnone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1552\u001b[0m     \"\"\"\n\u001b[1;32m-> 1553\u001b[1;33m     \u001b[0m_run_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1555\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/stream/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[1;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   3682\u001b[0m                        \u001b[1;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3683\u001b[0m                        \"graph.\")\n\u001b[1;32m-> 3684\u001b[1;33m   \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/stream/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 382\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    383\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/stream/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    653\u001b[0m     \u001b[0mmovers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_with_movers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m     results = self._do_run(handle, target_list, unique_fetches,\n\u001b[1;32m--> 655\u001b[1;33m                            feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[1;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/stream/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    721\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    722\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 723\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m    724\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/home/stream/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    728\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 730\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    731\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    732\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/stream/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m    710\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m    711\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 712\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m    713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k=0\n",
    "WeiDecayCount=0\n",
    "gamma=0.02\n",
    "BaseLearningR=1.7e-7\n",
    "#maxinterval=1\n",
    "#interval=0\n",
    "\n",
    "learningR=BaseLearningR\n",
    "for i in range(1,10000):\n",
    "  arr=np.arange(len(data['X_train']))\n",
    "  np.random.shuffle(arr)\n",
    "  idx = arr[:30]\n",
    "  train_step.run(feed_dict={x_tfed:data['X_train'][idx], y_tf: data['y_train'][idx],keep_prob:0.5,learningRate:learningR,regularTerm:1e-3})\n",
    "  \n",
    "  if i%100==0:\n",
    "    print(\"measurement\")\n",
    "    #interval+=1\n",
    "    ############testing acc#########\n",
    "    testing_accuracy,testing_cross_entropy = sess.run([accuracy,cross_entropy], feed_dict={   \n",
    "        x_tfed:data['X_val'], y_tf: data['y_val'],keep_prob:1})\n",
    "    ############ Wegiht Decay #########\n",
    "    WeiDecayCount+=1\n",
    "    learningR=BaseLearningR*np.exp(-gamma*WeiDecayCount)\n",
    "    print(\"==========Weight have been decaied %g==============\"%learningR)\n",
    "\n",
    "    if(mini_testing_cross_entropy>testing_cross_entropy):\n",
    "        #interval=0\n",
    "        mini_testing_cross_entropy=testing_cross_entropy\n",
    "        saver=tf.train.Saver(tf.all_variables())\n",
    "        saver.save(sess,\"resnet50.ckpt\")\n",
    "        print(\"**************minimum entropy updated %g **************\"%mini_testing_cross_entropy)\n",
    "    ############training acc###########\n",
    "    if i%1000==0:\n",
    "        train_accuracy = sess.run(accuracy, feed_dict={   \n",
    "                x_tfed:data['X_train'], y_tf: data['y_train'],keep_prob:1})\n",
    "        print(\"step %d, training accuracy %g, testing acc %g \\n,=== cross_entropy %g ===\"%(i, train_accuracy,testing_accuracy,testing_cross_entropy))\n",
    "    else:\n",
    "        print(\"step %d, testing acc %g, cross_entropy %g\"%(i,testing_accuracy,testing_cross_entropy))\n",
    "        \n",
    "#overall\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x_tfed:data['X_val'], y_tf: data['y_val'],keep_prob:1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.072975889"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(cross_entropy, feed_dict={x_tfed:data['X_val'], y_tf: data['y_val'],keep_prob:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'resnet50.ckpt'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver=tf.train.Saver(tf.all_variables())\n",
    "saver.save(sess,\"resnet50.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# can achieve 96.8~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [sec5-2]Tune from resnet 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "sess = tf.InteractiveSession()\n",
    "graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load preprocessed data\n",
    "feature_x=np.load(\"/home/stream/whimh2.0/Bibi/outfile_x.npy\")\n",
    "y_train=np.load(\"/home/stream/whimh2.0/Bibi/outfile_y.npy\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_tf = tf.placeholder(tf.float32, shape=[None, 64,64,3])\n",
    "#since the original image is 224x224 , we can resize it as the same or smaller(64x64)\n",
    "#just 32x32 will fails in some layers(since polling/conv).\n",
    "#x_tf_1=tf.image.resize_images(x_tf,64,64)\n",
    "x_tf_1=tf.image.resize_images(x_tf,224,224)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frames=sess.run(x_tf_1, feed_dict={x_tf:(feature_x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del feature_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsaver=tf.train.import_meta_graph('/home/stream/tensorflow-resnet-pretrained-20160509/ResNet-L101.meta')\n",
    "newsaver.restore(sess,'/home/stream/tensorflow-resnet-pretrained-20160509/ResNet-L101.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save the current graph and see what happens\n",
    "train_writer = tf.train.SummaryWriter('/tmp/resnet101/train',sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.get_default_graph()\n",
    "#[i.name for i in graph.get_operations()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_net=graph.get_tensor_by_name(\"scale5/block3/Relu:0\")\n",
    "#feature_net=graph.get_tensor_by_name(\"mul:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images=graph.get_tensor_by_name(\"images:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take a look at this\n",
    "print images\n",
    "print feature_net\n",
    "print frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_x=sess.run(feature_net, feed_dict={images:(frames[:1000]/255)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print feature_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time\n",
    "for i in range(1,25):\n",
    "    if i%5==0:\n",
    "        print \"current i: \"+str(i)\n",
    "    feature_temp = sess.run([feature_net], feed_dict={images:frames[i*1000:(i+1)*1000]/255})[0]\n",
    "    feature_x=np.concatenate((feature_x, feature_temp), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.save(\"/home/stream/whimh2.0/Bibi/finetune_res_101_scale5\",feature_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# [sec 5-3] fine tune resnet101\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_x=np.load(\"/home/stream/whimh2.0/Bibi/finetune_res_101_scale5.npy\")\n",
    "y_train=np.load(\"/home/stream/whimh2.0/Bibi/outfile_y.npy\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 7, 7, 2048)\n"
     ]
    }
   ],
   "source": [
    "print feature_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 20000 images in training set\n",
      "there are 5000 images in testing set\n"
     ]
    }
   ],
   "source": [
    "#arr = np.arange(len(feature_x))\n",
    "#np.random.shuffle(arr)\n",
    "arr=np.load(\"/home/stream/whimh2.0/Bibi/20161020seed.npy\")\n",
    "\n",
    "x_train=feature_x[arr]\n",
    "y_train=y_train[arr]\n",
    "data={\n",
    "  'X_train': x_train[:int(len(x_train)*0.8)],\n",
    "  'y_train': y_train[:int(len(x_train)*0.8)],\n",
    "  'X_val': x_train[int(len(x_train)*0.8):],\n",
    "  'y_val': y_train[int(len(x_train)*0.8):],\n",
    "}\n",
    "print \"there are \"+ str(data['X_train'].shape[0]) + \" images in training set\"\n",
    "print \"there are \"+ str(data['X_val'].shape[0]) + \" images in testing set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del feature_x\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.0001)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.01, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_tfed = tf.placeholder(tf.float32, shape=[None,7,7,2048],name='feature_x')\n",
    "y_tf = tf.placeholder(tf.int32, shape=[None,],name='truth_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Here contains the finetune layers\n",
    "with tf.name_scope('fintune_whimh'):\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    regularTerm = tf.placeholder(tf.float32)\n",
    "    learningRate = tf.placeholder(tf.float32)\n",
    "\n",
    "    with tf.name_scope('Fc1'):\n",
    "        #calculate_entropy\n",
    "        h_pool3_flat = tf.reshape(x_tfed, [-1, 7*7*2048])\n",
    "        W_whimh_fc1 = weight_variable([7*7*2048,1024])\n",
    "        b_whimh_fc1 = bias_variable([1024])\n",
    "        h_fc1=tf.nn.relu(tf.matmul(h_pool3_flat, W_whimh_fc1) + b_whimh_fc1) \n",
    "        #y_conv=tf.matmul(h_pool3_flat, W_whimh_fc1) + b_whimh_fc1\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "    with tf.name_scope('Fc2'):\n",
    "        #calculate_entropy\n",
    "        W_whimh_fc2 = weight_variable([1024, 2])\n",
    "        b_whimh_fc2 = bias_variable([2])\n",
    "        y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_whimh_fc2) + b_whimh_fc2, name='predictions_softmax')\n",
    "#        h_fc2=tf.nn.relu(tf.matmul(h_fc1_drop, W_whimh_fc2) + b_whimh_fc2) \n",
    "#        h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\n",
    "#    with tf.name_scope('Fc3'):\n",
    "        #calculate_entropy\n",
    "#        W_whimh_fc3 = weight_variable([512, 2])\n",
    "#        b_whimh_fc3 = bias_variable([2])\n",
    "#        y_conv=tf.nn.softmax(tf.matmul(h_fc2_drop, W_whimh_fc3) + b_whimh_fc3, name='predictions_softmax') \n",
    "    y_tf_2=tf.one_hot(y_tf,2)\n",
    "    with tf.name_scope('loss_calulate'):\n",
    "        #calculate_entropy\n",
    "        cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_tf_2 * tf.log(y_conv), reduction_indices=[1]))\n",
    "    with tf.name_scope('regular'):    \n",
    "        regularizers = (tf.nn.l2_loss(W_whimh_fc1) + tf.nn.l2_loss(b_whimh_fc1) +\n",
    "                        tf.nn.l2_loss(W_whimh_fc2) + tf.nn.l2_loss(b_whimh_fc2) )\n",
    "                        #tf.nn.l2_loss(W_whimh_fc3) + tf.nn.l2_loss(b_whimh_fc3) )\n",
    "    with tf.name_scope('loss'):\n",
    "        #carefully deal overfitting\n",
    "        loss=cross_entropy+regularTerm*regularizers\n",
    "    with tf.name_scope('slover'):\n",
    "        train_step = tf.train.AdamOptimizer(learningRate).minimize(loss)\n",
    "    with tf.name_scope('measure'):\n",
    "        with tf.name_scope('predict'):\n",
    "            predctions=tf.argmax(y_conv,1, name='predictions_')\n",
    "        with tf.name_scope('groundtruth'):\n",
    "            ground_truth=tf.argmax(y_tf_2,1)\n",
    "        correct_prediction = tf.equal(predctions, ground_truth)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testing_accuracy,testing_cross_entropy = sess.run([accuracy,cross_entropy], feed_dict={   \n",
    "    x_tfed:data['X_val'], y_tf: data['y_val'],keep_prob:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mini_testing_cross_entropy=testing_cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum entropy : 0.693147\n"
     ]
    }
   ],
   "source": [
    "print(\"minimum entropy : %g\"%mini_testing_cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "measurement\n",
      "==========Weight have been decaied 4.85223e-06==============\n",
      "**************minimum entropy updated 0.196437 **************\n",
      "step 100, testing acc 0.9612, cross_entropy 0.196437\n",
      "measurement\n",
      "==========Weight have been decaied 4.70882e-06==============\n",
      "**************minimum entropy updated 0.110937 **************\n",
      "step 200, testing acc 0.9664, cross_entropy 0.110937\n",
      "measurement\n",
      "==========Weight have been decaied 4.56966e-06==============\n",
      "**************minimum entropy updated 0.0943852 **************\n",
      "step 300, testing acc 0.9666, cross_entropy 0.0943852\n",
      "measurement\n",
      "==========Weight have been decaied 4.4346e-06==============\n",
      "**************minimum entropy updated 0.0852987 **************\n",
      "step 400, testing acc 0.968, cross_entropy 0.0852987\n",
      "measurement\n",
      "==========Weight have been decaied 4.30354e-06==============\n",
      "**************minimum entropy updated 0.0806429 **************\n",
      "step 500, testing acc 0.9706, cross_entropy 0.0806429\n",
      "measurement\n",
      "==========Weight have been decaied 4.17635e-06==============\n",
      "**************minimum entropy updated 0.0779967 **************\n",
      "step 600, testing acc 0.9708, cross_entropy 0.0779967\n",
      "measurement\n",
      "==========Weight have been decaied 4.05292e-06==============\n",
      "**************minimum entropy updated 0.0778117 **************\n",
      "step 700, testing acc 0.9712, cross_entropy 0.0778117\n",
      "measurement\n",
      "==========Weight have been decaied 3.93314e-06==============\n",
      "**************minimum entropy updated 0.0755167 **************\n",
      "step 800, testing acc 0.9714, cross_entropy 0.0755167\n",
      "measurement\n",
      "==========Weight have been decaied 3.8169e-06==============\n",
      "**************minimum entropy updated 0.0741534 **************\n",
      "step 900, testing acc 0.9732, cross_entropy 0.0741534\n",
      "measurement\n",
      "==========Weight have been decaied 3.70409e-06==============\n",
      "**************minimum entropy updated 0.0737449 **************\n",
      "step 1000, training accuracy 0.98255, testing acc 0.973 \n",
      ",=== cross_entropy 0.0737449 ===\n",
      "measurement\n",
      "==========Weight have been decaied 3.59462e-06==============\n",
      "**************minimum entropy updated 0.0720442 **************\n",
      "step 1100, testing acc 0.9726, cross_entropy 0.0720442\n",
      "measurement\n",
      "==========Weight have been decaied 3.48838e-06==============\n",
      "**************minimum entropy updated 0.0717738 **************\n",
      "step 1200, testing acc 0.9722, cross_entropy 0.0717738\n",
      "measurement\n",
      "==========Weight have been decaied 3.38528e-06==============\n",
      "step 1300, testing acc 0.97, cross_entropy 0.0722956\n",
      "measurement\n",
      "==========Weight have been decaied 3.28523e-06==============\n",
      "step 1400, testing acc 0.9728, cross_entropy 0.0717892\n",
      "measurement\n",
      "==========Weight have been decaied 3.18814e-06==============\n",
      "step 1500, testing acc 0.9734, cross_entropy 0.0746089\n",
      "measurement\n",
      "==========Weight have been decaied 3.09392e-06==============\n",
      "**************minimum entropy updated 0.0712547 **************\n",
      "step 1600, testing acc 0.973, cross_entropy 0.0712547\n",
      "measurement\n",
      "==========Weight have been decaied 3.00248e-06==============\n",
      "step 1700, testing acc 0.973, cross_entropy 0.0722253\n",
      "measurement\n",
      "==========Weight have been decaied 2.91374e-06==============\n",
      "**************minimum entropy updated 0.0703927 **************\n",
      "step 1800, testing acc 0.973, cross_entropy 0.0703927\n",
      "measurement\n",
      "==========Weight have been decaied 2.82763e-06==============\n",
      "**************minimum entropy updated 0.0697645 **************\n",
      "step 1900, testing acc 0.9732, cross_entropy 0.0697645\n",
      "measurement\n",
      "==========Weight have been decaied 2.74406e-06==============\n",
      "**************minimum entropy updated 0.0695401 **************\n",
      "step 2000, training accuracy 0.9902, testing acc 0.9728 \n",
      ",=== cross_entropy 0.0695401 ===\n",
      "measurement\n",
      "==========Weight have been decaied 2.66296e-06==============\n",
      "**************minimum entropy updated 0.0690426 **************\n",
      "step 2100, testing acc 0.9722, cross_entropy 0.0690426\n",
      "measurement\n",
      "==========Weight have been decaied 2.58426e-06==============\n",
      "step 2200, testing acc 0.9738, cross_entropy 0.0695388\n",
      "measurement\n",
      "==========Weight have been decaied 2.50788e-06==============\n",
      "**************minimum entropy updated 0.0689996 **************\n",
      "step 2300, testing acc 0.9728, cross_entropy 0.0689996\n",
      "measurement\n",
      "==========Weight have been decaied 2.43376e-06==============\n",
      "**************minimum entropy updated 0.0689698 **************\n",
      "step 2400, testing acc 0.9734, cross_entropy 0.0689698\n",
      "measurement\n",
      "==========Weight have been decaied 2.36183e-06==============\n",
      "step 2500, testing acc 0.973, cross_entropy 0.06956\n",
      "measurement\n",
      "==========Weight have been decaied 2.29203e-06==============\n",
      "step 2600, testing acc 0.9744, cross_entropy 0.0693811\n",
      "measurement\n",
      "==========Weight have been decaied 2.22429e-06==============\n",
      "step 2700, testing acc 0.9744, cross_entropy 0.0704646\n",
      "measurement\n",
      "==========Weight have been decaied 2.15855e-06==============\n",
      "step 2800, testing acc 0.9738, cross_entropy 0.070323\n",
      "measurement\n",
      "==========Weight have been decaied 2.09476e-06==============\n",
      "step 2900, testing acc 0.9736, cross_entropy 0.069544\n",
      "measurement\n",
      "==========Weight have been decaied 2.03285e-06==============\n",
      "step 3000, training accuracy 0.99505, testing acc 0.973 \n",
      ",=== cross_entropy 0.0694046 ===\n",
      "measurement\n",
      "==========Weight have been decaied 1.97277e-06==============\n",
      "step 3100, testing acc 0.9724, cross_entropy 0.0693327\n",
      "measurement\n",
      "==========Weight have been decaied 1.91446e-06==============\n",
      "step 3200, testing acc 0.974, cross_entropy 0.0692959\n",
      "measurement\n",
      "==========Weight have been decaied 1.85788e-06==============\n",
      "step 3300, testing acc 0.9736, cross_entropy 0.0694769\n",
      "measurement\n",
      "==========Weight have been decaied 1.80297e-06==============\n",
      "step 3400, testing acc 0.9736, cross_entropy 0.0732155\n",
      "measurement\n",
      "==========Weight have been decaied 1.74969e-06==============\n",
      "step 3500, testing acc 0.973, cross_entropy 0.0713908\n",
      "measurement\n",
      "==========Weight have been decaied 1.69798e-06==============\n",
      "step 3600, testing acc 0.973, cross_entropy 0.0714628\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-63b8004c46d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m   \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m   \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m   \u001b[0mtrain_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx_tfed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'X_train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_tf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y_train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearningRate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlearningR\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mregularTerm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2e-3\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/stream/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m   1551\u001b[0m         \u001b[0mnone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1552\u001b[0m     \"\"\"\n\u001b[1;32m-> 1553\u001b[1;33m     \u001b[0m_run_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1555\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/stream/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[1;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   3682\u001b[0m                        \u001b[1;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3683\u001b[0m                        \"graph.\")\n\u001b[1;32m-> 3684\u001b[1;33m   \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/stream/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 382\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    383\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/stream/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    653\u001b[0m     \u001b[0mmovers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_with_movers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m     results = self._do_run(handle, target_list, unique_fetches,\n\u001b[1;32m--> 655\u001b[1;33m                            feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[1;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/stream/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    721\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    722\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 723\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m    724\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/home/stream/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    728\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 730\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    731\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    732\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/stream/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m    710\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m    711\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 712\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m    713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k=0\n",
    "WeiDecayCount=0\n",
    "gamma=0.03\n",
    "BaseLearningR=5e-6\n",
    "#maxinterval=1\n",
    "#interval=0\n",
    "\n",
    "learningR=BaseLearningR\n",
    "for i in range(1,10000):\n",
    "  arr=np.arange(len(data['X_train']))\n",
    "  np.random.shuffle(arr)\n",
    "  idx = arr[:30]\n",
    "  train_step.run(feed_dict={x_tfed:data['X_train'][idx], y_tf: data['y_train'][idx],keep_prob:0.5,learningRate:learningR,regularTerm:2e-3})\n",
    "  \n",
    "  if i%100==0:\n",
    "    print(\"measurement\")\n",
    "    #interval+=1\n",
    "    ############testing acc#########\n",
    "    testing_accuracy,testing_cross_entropy = sess.run([accuracy,cross_entropy], feed_dict={   \n",
    "        x_tfed:data['X_val'], y_tf: data['y_val'],keep_prob:1})\n",
    "    ############ Wegiht Decay #########\n",
    "    WeiDecayCount+=1\n",
    "    learningR=BaseLearningR*np.exp(-gamma*WeiDecayCount)\n",
    "    print(\"==========Weight have been decaied %g==============\"%learningR)\n",
    "\n",
    "    if(mini_testing_cross_entropy>testing_cross_entropy):\n",
    "        #interval=0\n",
    "        mini_testing_cross_entropy=testing_cross_entropy\n",
    "        saver=tf.train.Saver(tf.all_variables())\n",
    "        saver.save(sess,\"resnet101.ckpt\")\n",
    "        print(\"**************minimum entropy updated %g **************\"%mini_testing_cross_entropy)\n",
    "    ############training acc###########\n",
    "    if i%1000==0:\n",
    "        train_accuracy = sess.run(accuracy, feed_dict={   \n",
    "                x_tfed:data['X_train'], y_tf: data['y_train'],keep_prob:1})\n",
    "        print(\"step %d, training accuracy %g, testing acc %g \\n,=== cross_entropy %g ===\"%(i, train_accuracy,testing_accuracy,testing_cross_entropy))\n",
    "    else:\n",
    "        print(\"step %d, testing acc %g, cross_entropy %g\"%(i,testing_accuracy,testing_cross_entropy))\n",
    "        \n",
    "#overall\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x_tfed:data['X_val'], y_tf: data['y_val'],keep_prob:1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.089114882"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(cross_entropy, feed_dict={x_tfed:data['X_val'], y_tf: data['y_val'],keep_prob:1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum entropy : 0.0689698\n"
     ]
    }
   ],
   "source": [
    "print(\"minimum entropy : %g\"%mini_testing_cross_entropy)\n",
    "#resnet F1,F2,F3 can achieve 97.4~\n",
    "#resnet F1,F2 can acheive 97.46~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sec[5-5] Resnet 151"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "sess = tf.InteractiveSession()\n",
    "graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load preprocessed data\n",
    "feature_x=np.load(\"/home/stream/whimh2.0/Bibi/outfile_x.npy\")\n",
    "y_train=np.load(\"/home/stream/whimh2.0/Bibi/outfile_y.npy\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_tf = tf.placeholder(tf.float32, shape=[None, 64,64,3])\n",
    "#since the original image is 224x224 , we can resize it as the same or smaller(64x64)\n",
    "#just 32x32 will fails in some layers(since polling/conv).\n",
    "#x_tf_1=tf.image.resize_images(x_tf,64,64)\n",
    "x_tf_1=tf.image.resize_images(x_tf,224,224)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frames=sess.run(x_tf_1, feed_dict={x_tf:(feature_x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del feature_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsaver=tf.train.import_meta_graph('/home/stream/tensorflow-resnet-pretrained-20160509/ResNet-L152.meta')\n",
    "newsaver.restore(sess,'/home/stream/tensorflow-resnet-pretrained-20160509/ResNet-L152.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save the current graph and see what happens\n",
    "train_writer = tf.train.SummaryWriter('/tmp/resnet152/train',sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.get_default_graph()\n",
    "#[i.name for i in graph.get_operations()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_net=graph.get_tensor_by_name(\"scale5/block3/Relu:0\")\n",
    "#feature_net=graph.get_tensor_by_name(\"mul:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images=graph.get_tensor_by_name(\"images:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take a look at this\n",
    "print images\n",
    "print feature_net\n",
    "print frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_x=sess.run(feature_net, feed_dict={images:(frames[:1000]/255)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print feature_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time\n",
    "for i in range(1,25):\n",
    "    if i%5==0:\n",
    "        print \"current i: \"+str(i)\n",
    "    feature_temp = sess.run([feature_net], feed_dict={images:frames[i*1000:(i+1)*1000]/255})[0]\n",
    "    feature_x=np.concatenate((feature_x, feature_temp), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.save(\"/home/stream/whimh2.0/Bibi/finetune_res_152_scale5\",feature_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# [sec 5-4] fine tune resnet151"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74081822068171788"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(-6*0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_x=np.load(\"/home/stream/whimh2.0/Bibi/finetune_res_152_scale5.npy\")\n",
    "y_train=np.load(\"/home/stream/whimh2.0/Bibi/outfile_y.npy\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 7, 7, 2048)\n"
     ]
    }
   ],
   "source": [
    "print feature_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are (20000, 7, 7, 2048) images in training set\n",
      "there are (5000, 7, 7, 2048) images in testing set\n"
     ]
    }
   ],
   "source": [
    "#arr = np.arange(len(feature_x))\n",
    "#np.random.shuffle(arr)\n",
    "arr=np.load(\"/home/stream/whimh2.0/Bibi/20161020seed.npy\")\n",
    "x_train=feature_x[arr]\n",
    "y_train=y_train[arr]\n",
    "data={\n",
    "  'X_train': x_train[:int(len(x_train)*0.8)],\n",
    "  'y_train': y_train[:int(len(x_train)*0.8)],\n",
    "  'X_val': x_train[int(len(x_train)*0.8):],\n",
    "  'y_val': y_train[int(len(x_train)*0.8):],\n",
    "}\n",
    "print \"there are \"+ str(data['X_train'].shape) + \" images in training set\"\n",
    "print \"there are \"+ str(data['X_val'].shape) + \" images in testing set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del feature_x\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.0001)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.01, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_tfed = tf.placeholder(tf.float32, shape=[None,7,7,2048],name='feature_x')\n",
    "y_tf = tf.placeholder(tf.int32, shape=[None,],name='truth_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Here contains the finetune layers\n",
    "with tf.name_scope('fintune_whimh'):\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    regularTerm = tf.placeholder(tf.float32)\n",
    "    learningRate = tf.placeholder(tf.float32)\n",
    "\n",
    "    with tf.name_scope('Fc1'):\n",
    "        #calculate_entropy\n",
    "        h_pool3_flat = tf.reshape(x_tfed, [-1, 7*7*2048])\n",
    "        W_whimh_fc1 = weight_variable([7*7*2048,1024])\n",
    "        b_whimh_fc1 = bias_variable([1024])\n",
    "        h_fc1=tf.nn.relu(tf.matmul(h_pool3_flat, W_whimh_fc1) + b_whimh_fc1) \n",
    "        #y_conv=tf.matmul(h_pool3_flat, W_whimh_fc1) + b_whimh_fc1\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "    with tf.name_scope('Fc2'):\n",
    "        #calculate_entropy\n",
    "        W_whimh_fc2 = weight_variable([1024, 2])\n",
    "        b_whimh_fc2 = bias_variable([2])\n",
    "        y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_whimh_fc2) + b_whimh_fc2, name='predictions_softmax')\n",
    "#        h_fc2=tf.nn.relu(tf.matmul(h_fc1_drop, W_whimh_fc2) + b_whimh_fc2) \n",
    "#        h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\n",
    "#    with tf.name_scope('Fc3'):\n",
    "        #calculate_entropy\n",
    "#        W_whimh_fc3 = weight_variable([512, 2])\n",
    "#        b_whimh_fc3 = bias_variable([2])\n",
    "#        y_conv=tf.nn.softmax(tf.matmul(h_fc2_drop, W_whimh_fc3) + b_whimh_fc3, name='predictions_softmax') \n",
    "    y_tf_2=tf.one_hot(y_tf,2)\n",
    "    with tf.name_scope('loss_calulate'):\n",
    "        #calculate_entropy\n",
    "        cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_tf_2 * tf.log(y_conv), reduction_indices=[1]))\n",
    "    with tf.name_scope('regular'):    \n",
    "        regularizers = (tf.nn.l2_loss(W_whimh_fc1) + tf.nn.l2_loss(b_whimh_fc1) +\n",
    "                        tf.nn.l2_loss(W_whimh_fc2) + tf.nn.l2_loss(b_whimh_fc2) )\n",
    "                        #tf.nn.l2_loss(W_whimh_fc3) + tf.nn.l2_loss(b_whimh_fc3) )\n",
    "    with tf.name_scope('loss'):\n",
    "        #carefully deal overfitting\n",
    "        loss=cross_entropy+regularTerm*regularizers\n",
    "    with tf.name_scope('slover'):\n",
    "        train_step = tf.train.AdamOptimizer(learningRate).minimize(loss)\n",
    "    with tf.name_scope('measure'):\n",
    "        with tf.name_scope('predict'):\n",
    "            predctions=tf.argmax(y_conv,1, name='predictions_')\n",
    "        with tf.name_scope('groundtruth'):\n",
    "            ground_truth=tf.argmax(y_tf_2,1)\n",
    "        correct_prediction = tf.equal(predctions, ground_truth)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testing_accuracy,testing_cross_entropy = sess.run([accuracy,cross_entropy], feed_dict={   \n",
    "    x_tfed:data['X_val'], y_tf: data['y_val'],keep_prob:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mini_testing_cross_entropy=testing_cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum entropy : 0.693155\n"
     ]
    }
   ],
   "source": [
    "print(\"minimum entropy : %g\"%mini_testing_cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "measurement\n",
      "==========Weight have been decaied 4.85223e-06==============\n",
      "**************minimum entropy updated 0.184972 **************\n",
      "step 100, testing acc 0.9574, cross_entropy 0.184972\n"
     ]
    }
   ],
   "source": [
    "k=0\n",
    "WeiDecayCount=0\n",
    "gamma=0.03\n",
    "BaseLearningR=5e-6\n",
    "#maxinterval=1\n",
    "#interval=0\n",
    "\n",
    "learningR=BaseLearningR\n",
    "for i in range(1,10000):\n",
    "  arr=np.arange(len(data['X_train']))\n",
    "  np.random.shuffle(arr)\n",
    "  idx = arr[:30]\n",
    "  train_step.run(feed_dict={x_tfed:data['X_train'][idx], y_tf: data['y_train'][idx],keep_prob:0.5,learningRate:learningR,regularTerm:2e-3})\n",
    "  \n",
    "  if i%100==0:\n",
    "    print(\"measurement\")\n",
    "    #interval+=1\n",
    "    ############testing acc#########\n",
    "    testing_accuracy,testing_cross_entropy = sess.run([accuracy,cross_entropy], feed_dict={   \n",
    "        x_tfed:data['X_val'], y_tf: data['y_val'],keep_prob:1})\n",
    "    ############ Wegiht Decay #########\n",
    "    WeiDecayCount+=1\n",
    "    learningR=BaseLearningR*np.exp(-gamma*WeiDecayCount)\n",
    "    print(\"==========Weight have been decaied %g==============\"%learningR)\n",
    "\n",
    "    if(mini_testing_cross_entropy>testing_cross_entropy):\n",
    "        #interval=0\n",
    "        mini_testing_cross_entropy=testing_cross_entropy\n",
    "        saver=tf.train.Saver(tf.all_variables())\n",
    "        saver.save(sess,\"resnet152.ckpt\")\n",
    "        print(\"**************minimum entropy updated %g **************\"%mini_testing_cross_entropy)\n",
    "    ############training acc###########\n",
    "    if i%1000==0:\n",
    "        train_accuracy = sess.run(accuracy, feed_dict={   \n",
    "                x_tfed:data['X_train'], y_tf: data['y_train'],keep_prob:1})\n",
    "        print(\"step %d, training accuracy %g, testing acc %g \\n,=== cross_entropy %g ===\"%(i, train_accuracy,testing_accuracy,testing_cross_entropy))\n",
    "    else:\n",
    "        print(\"step %d, testing acc %g, cross_entropy %g\"%(i,testing_accuracy,testing_cross_entropy))\n",
    "        \n",
    "#overall\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x_tfed:data['X_val'], y_tf: data['y_val'],keep_prob:1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"minimum entropy : %g\"%mini_testing_cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13115662"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(cross_entropy, feed_dict={x_tfed:data['X_val'], y_tf: data['y_val'],keep_prob:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Resnet152.ckpt'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver=tf.train.Saver(tf.all_variables())\n",
    "saver.save(sess,\"Resnet152.ckpt\")\n",
    "#resnet F1,F2 can acheive 97.62~\n",
    "#cross entroy ~0.06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# close it to clear graph\n",
    "tf.reset_default_graph()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test_set_Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "datapath=\"/home/stream/Downloads/cat_dog_data_all/test/\"\n",
    "\n",
    "pic_list=glob.glob(datapath+\"*.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w=[int(i.split('/')[-1].split('.')[0]) for i in pic_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frames = np.empty((len(pic_list), 64,64, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for k in xrange(len(pic_list)):\n",
    "    frames[k,:,:,:] = cv2.cvtColor(cv2.resize(cv2.imread(pic_list[k]),(64,64)), cv2.cv.CV_BGR2RGB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert len(frames)==len(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.save(\"/home/stream/whimh2.0/Bibi/test_ID\",w)\n",
    "np.save(\"/home/stream/whimh2.0/Bibi/test_X\",frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# restore sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsaver=tf.train.import_meta_graph('/home/stream/whimh2.0/Bibi/trained_model/Resnet152.ckpt.meta')\n",
    "newsaver.restore(sess,'/home/stream/whimh2.0/Bibi/trained_model/Resnet152.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_x=np.load(\"/home/stream/whimh2.0/Bibi/test_res_152_scale5.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_ID=np.load(\"/home/stream/whimh2.0/Bibi/test_ID.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_ID[:5]\n",
    "#dog,cat,cat,dog,dog\n",
    "#1,0,0,1,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#[i.name for i in graph.get_operations()]\n",
    "graph = tf.get_default_graph()\n",
    "predctions=graph.get_tensor_by_name(\"fintune_whimh/measure/predict/predictions_:0\")\n",
    "x_tfed=graph.get_tensor_by_name(\"feature_x:0\")\n",
    "keep_prob=graph.get_tensor_by_name(\"fintune_whimh/Placeholder:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print keep_prob\n",
    "print x_tfed\n",
    "print predctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w=sess.run(predctions, feed_dict={x_tfed:feature_x[:5],keep_prob:1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [sec6] mixture all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_x=np.empty((100352*2,25000),dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_x[:100352]=np.load(\"/home/stream/whimh2.0/Bibi/finetune_conv5_2.npy\").reshape(25000,100352).transpose(1,0)\n",
    "feature_x[100352:100352*2]=np.load(\"/home/stream/whimh2.0/Bibi/finetune_res_152_scale5.npy\").reshape(25000,100352).transpose(1,0)\n",
    "\n",
    "#feature_50=np.load(\"/home/stream/whimh2.0/Bibi/finetune_res_50_scale5.npy\")\n",
    "#feature_152=np.load(\"/home/stream/whimh2.0/Bibi/finetune_res_152_scale5.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_x=feature_x.transpose(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train=np.load(\"/home/stream/whimh2.0/Bibi/outfile_y.npy\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "print y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are (20000, 200704) images in training set\n",
      "there are (5000, 200704) images in testing set\n"
     ]
    }
   ],
   "source": [
    "arr = np.arange(len(feature_x))\n",
    "np.random.shuffle(arr)\n",
    "x_train=feature_x[arr]\n",
    "y_train=y_train[arr]\n",
    "data={\n",
    "  'X_train': x_train[:int(len(x_train)*0.8)],\n",
    "  'y_train': y_train[:int(len(x_train)*0.8)],\n",
    "  'X_val': x_train[int(len(x_train)*0.8):],\n",
    "  'y_val': y_train[int(len(x_train)*0.8):],\n",
    "}\n",
    "print \"there are \"+ str(data['X_train'].shape) + \" images in training set\"\n",
    "print \"there are \"+ str(data['X_val'].shape) + \" images in testing set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del feature_x\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.0001)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.01, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_tfed = tf.placeholder(tf.float32, shape=[None,200704],name='feature_x')\n",
    "y_tf = tf.placeholder(tf.int32, shape=[None,],name='truth_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Here contains the finetune layers\n",
    "with tf.name_scope('fintune_whimh'):\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    regularTerm = tf.placeholder(tf.float32)\n",
    "    learningRate = tf.placeholder(tf.float32)\n",
    "\n",
    "    with tf.name_scope('Fc1'):\n",
    "        #calculate_entropy\n",
    "        W_whimh_fc1 = weight_variable([7*7*2048*2,512])\n",
    "        b_whimh_fc1 = bias_variable([512])\n",
    "        h_fc1=tf.nn.relu(tf.matmul(x_tfed, W_whimh_fc1) + b_whimh_fc1) \n",
    "        #y_conv=tf.matmul(h_pool3_flat, W_whimh_fc1) + b_whimh_fc1\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "    with tf.name_scope('Fc2'):\n",
    "        #calculate_entropy\n",
    "        W_whimh_fc2 = weight_variable([512, 2])\n",
    "        b_whimh_fc2 = bias_variable([2])\n",
    "        y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_whimh_fc2) + b_whimh_fc2, name='predictions_softmax')\n",
    "#        h_fc2=tf.nn.relu(tf.matmul(h_fc1_drop, W_whimh_fc2) + b_whimh_fc2) \n",
    "#        h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\n",
    "#    with tf.name_scope('Fc3'):\n",
    "        #calculate_entropy\n",
    "#        W_whimh_fc3 = weight_variable([512, 2])\n",
    "#        b_whimh_fc3 = bias_variable([2])\n",
    "#        y_conv=tf.nn.softmax(tf.matmul(h_fc2_drop, W_whimh_fc3) + b_whimh_fc3, name='predictions_softmax') \n",
    "    y_tf_2=tf.one_hot(y_tf,2)\n",
    "    with tf.name_scope('loss_calulate'):\n",
    "        #calculate_entropy\n",
    "        cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_tf_2 * tf.log(y_conv), reduction_indices=[1]))\n",
    "    with tf.name_scope('regular'):    \n",
    "        regularizers = (tf.nn.l2_loss(W_whimh_fc1) + tf.nn.l2_loss(b_whimh_fc1) +\n",
    "                        tf.nn.l2_loss(W_whimh_fc2) + tf.nn.l2_loss(b_whimh_fc2) )\n",
    "                        #tf.nn.l2_loss(W_whimh_fc3) + tf.nn.l2_loss(b_whimh_fc3) )\n",
    "    with tf.name_scope('loss'):\n",
    "        #carefully deal overfitting\n",
    "        loss=cross_entropy+regularTerm*regularizers\n",
    "    with tf.name_scope('slover'):\n",
    "        train_step = tf.train.AdamOptimizer(learningRate).minimize(loss)\n",
    "    with tf.name_scope('measure'):\n",
    "        with tf.name_scope('predict'):\n",
    "            predctions=tf.argmax(y_conv,1, name='predictions_')\n",
    "        with tf.name_scope('groundtruth'):\n",
    "            ground_truth=tf.argmax(y_tf_2,1)\n",
    "        correct_prediction = tf.equal(predctions, ground_truth)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k=0\n",
    "for i in range(1,20000):\n",
    "  arr=np.arange(len(data['X_train']))\n",
    "  np.random.shuffle(arr)\n",
    "  idx = arr[:30]\n",
    "  train_step.run(feed_dict={x_tfed:data['X_train'][idx], y_tf: data['y_train'][idx],keep_prob:0.5,learningRate:learningObject,regularTerm:1e-3})\n",
    "  \n",
    "  if i%100==0:\n",
    "    print(\"measurement\")\n",
    "    ############testing acc#########\n",
    "    testing_accuracy = sess.run(accuracy, feed_dict={   \n",
    "        x_tfed:data['X_val'], y_tf: data['y_val'],keep_prob:1})\n",
    "    ############training acc###########\n",
    "    if i%500==0:\n",
    "        train_accuracy = sess.run(accuracy, feed_dict={   \n",
    "                x_tfed:data['X_train'], y_tf: data['y_train'],keep_prob:1})\n",
    "        print(\"step %d, training accuracy %g, testing acc %g\"%(i, train_accuracy,testing_accuracy))\n",
    "    else:\n",
    "        print(\"step %d, testing acc %g\"%(i,testing_accuracy))\n",
    "        \n",
    "#overall\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x_tfed:data['X_val'], y_tf: data['y_val'],keep_prob:1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# close it to clear graph\n",
    "tf.reset_default_graph()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
